{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications import DenseNet121,DenseNet169,DenseNet201,MobileNetV2,VGG19\n",
    "import tensorflow\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import gc\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from skimage.color import rgb2hsv, lab2lch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from albumentations import *\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model, Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "# Load libraries\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Normal SVM\n",
    "\n",
    "#import all necessary libraries\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt  # doctest: +SKIP\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import os\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between test harness and ideal test condition\n",
    "from numpy import mean\n",
    "from numpy import isnan\n",
    "from numpy import asarray\n",
    "from numpy import polyfit\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Recall metric.\n",
    "    \n",
    "    Only computes a batch-wise average of recall.\n",
    "    \n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    \n",
    "    Only computes a batch-wise average of precision.\n",
    "    \n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisionx = precision(y_true, y_pred)\n",
    "    recallx = recall(y_true, y_pred)\n",
    "    return 2*((precisionx*recallx)/(precisionx+recallx+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=np.load(\"features/effb0_f.npy\")\n",
    "y_train1=np.load(\"features/effb0_y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape ,y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate a logistic regression model using k-fold cross-validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from scipy.io import loadmat,savemat\n",
    "savemat(\"feat.mat\",{\"feat\":f})\n",
    "savemat(\"label.mat\",{\"label\":y_train1})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, Y_train, Y_validation = train_test_split(f, y_train1, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, train_label = f,y_train1\n",
    "#val_feature, val_label = X_validation,Y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fea = pca.transform(train_feature)[:, :100]\n",
    "#val_fea = pca.transform(val_feature)[:, :100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal SVM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "#load the dataset and split it into training and testing sets\n",
    "\n",
    "# train the model on train set without using GridSearchCV \n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# create model\n",
    "#model = SVC() \n",
    "#model=RidgeClassifier()\n",
    "model=KNeighborsClassifier()\n",
    "#model=XGBClassifier()\n",
    "model.fit(train_fea, train_label) \n",
    "# evaluate model\n",
    "\n",
    "   \n",
    "# print prediction results \n",
    "predictions = model.predict(val_fea) \n",
    "print(classification_report(val_label, predictions)) \n",
    "class_names = [\"normal\",\"bening\",\"malign\"]\n",
    "\n",
    "plot_confusion_matrix(model, val_fea,val_label,display_labels=class_names,\n",
    "                                 cmap=plt.cm.Purples)  # doctest: +SKIP\n",
    "\n",
    "plt.savefig(\"KNNeff4pca.png\",dpi=300)\n",
    "\n",
    "plt.show()  # doctest: +SKIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "predictions = cross_val_predict(model, train_fea, train_label, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score,balanced_accuracy_score,precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "Acc=accuracy_score(val_label, model.predict(val_fea))\n",
    "prf=precision_recall_fscore_support(val_label, model.predict(val_fea), average='weighted')\n",
    "Kappa=cohen_kappa_score(val_label, model.predict(val_fea))\n",
    "#rf_auc = balanced_accuracy_score(val_label, model.predict(val_fea))\n",
    "MSE=metrics.mean_squared_error(val_label, model.predict(val_fea))\n",
    "RMSE=np.sqrt(metrics.mean_squared_error(val_label, model.predict(val_fea)))\n",
    "MAE=metrics.mean_absolute_error(val_label, model.predict(val_fea))\n",
    "    \n",
    "print(\"Acc: \"+ str(Acc),\"Kappa: \"+str(Kappa), \"MSE: \"+str(MSE),\"RMSE: \"+str(RMSE), \"MAE: \"+str(MAE),\"PRF:\",str(prf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_fea,train_label\n",
    "\n",
    "# The scorers can be either one of the predefined metric strings or a scorer\n",
    "# callable, like the one returned by make_scorer\n",
    "scoring = {\"Balanced Accuracy\": make_scorer(balanced_accuracy_score), \"Accuracy\": make_scorer(accuracy_score)}\n",
    "\n",
    "# Setting refit='AUC', refits an estimator on the whole dataset with the\n",
    "# parameter setting that has the best cross-validated AUC score.\n",
    "# That estimator is made available at ``gs.best_estimator_`` along with\n",
    "# parameters like ``gs.best_score_``, ``gs.best_params_`` and\n",
    "# ``gs.best_index_``\n",
    "gs = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid={\"min_samples_split\": range(2, 403, 10)},\n",
    "    scoring=scoring,\n",
    "    refit=\"Accuracy\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "gs.fit(X, y)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\", fontsize=16)\n",
    "\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(0, 402)\n",
    "ax.set_ylim(0.73, 1)\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results[\"param_min_samples_split\"].data, dtype=float)\n",
    "\n",
    "for scorer, color in zip(sorted(scoring), [\"g\", \"k\"]):\n",
    "    for sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\n",
    "        sample_score_mean = results[\"mean_%s_%s\" % (sample, scorer)]\n",
    "        sample_score_std = results[\"std_%s_%s\" % (sample, scorer)]\n",
    "        ax.fill_between(\n",
    "            X_axis,\n",
    "            sample_score_mean - sample_score_std,\n",
    "            sample_score_mean + sample_score_std,\n",
    "            alpha=0.1 if sample == \"test\" else 0,\n",
    "            color=color,\n",
    "        )\n",
    "        ax.plot(\n",
    "            X_axis,\n",
    "            sample_score_mean,\n",
    "            style,\n",
    "            color=color,\n",
    "            alpha=1 if sample == \"test\" else 0.7,\n",
    "            label=\"%s (%s)\" % (scorer, sample),\n",
    "        )\n",
    "\n",
    "    best_index = np.nonzero(results[\"rank_test_%s\" % scorer] == 1)[0][0]\n",
    "    best_score = results[\"mean_test_%s\" % scorer][best_index]\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot(\n",
    "        [\n",
    "            X_axis[best_index],\n",
    "        ]\n",
    "        * 2,\n",
    "        [0, best_score],\n",
    "        linestyle=\"-.\",\n",
    "        color=color,\n",
    "        marker=\"x\",\n",
    "        markeredgewidth=3,\n",
    "        ms=8,\n",
    "    )\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\"%0.2f\" % best_score, (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import time\n",
    "from sklearn.datasets import  load_iris\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "\n",
    "models = [GaussianNB(), DecisionTreeClassifier(), SVC()]\n",
    "names = [\"Naive Bayes\", \"Decision Tree\", \"SVM\"]\n",
    "\n",
    "def getScores(estimator, x, y):\n",
    "    yPred = estimator.predict(x)\n",
    "    return (accuracy_score(y, yPred), \n",
    "            precision_score(y, yPred, pos_label=3, average='macro'), \n",
    "            recall_score(y, yPred, pos_label=3, average='macro'))\n",
    "\n",
    "def my_scorer(estimator, x, y):\n",
    "    a, p, r = getScores(estimator, x, y)\n",
    "    print (a, p, r)\n",
    "    return a+p+r\n",
    "\n",
    "for model, name in zip(models, names):\n",
    "    print (name)\n",
    "    start = time.time()\n",
    "    m = cross_val_score(model, train_fea, train_label,scoring=my_scorer, cv=10).mean()\n",
    "    \n",
    "    print ('\\nSum:',mean(m), '\\n\\n')\n",
    "    print ('time', time.time() - start, '\\n\\n')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score,balanced_accuracy_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "Acc=accuracy_score(val_label, model.predict(val_fea))\n",
    "Kappa=cohen_kappa_score(val_label, model.predict(val_fea))\n",
    "rf_auc = balanced_accuracy_score(val_label, model.predict(val_fea))\n",
    "MSE=metrics.mean_squared_error(val_label, model.predict(val_fea))\n",
    "RMSE=np.sqrt(metrics.mean_squared_error(val_label, model.predict(val_fea)))\n",
    "MAE=metrics.mean_absolute_error(val_label, model.predict(val_fea))\n",
    "    \n",
    "print(\"Acc: \"+ str(Acc),\"Kappa: \"+str(Kappa),\"AUC: \"+str(rf_auc), \"MSE: \"+str(MSE),\"RMSE: \"+str(RMSE), \"MAE: \"+str(MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(val_label, model.predict(val_fea), weights=\"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier() \n",
    "model.fit(train_fea, train_label) \n",
    "   \n",
    "# print prediction results \n",
    "predictions = model.predict(val_fea) \n",
    "print(classification_report(val_label, predictions)) \n",
    "class_names = [\"normal\",\"bening\",\"malign\"]\n",
    "\n",
    "plot_confusion_matrix(model, val_fea,val_label,display_labels=class_names,\n",
    "                                 cmap=plt.cm.Purples,values_format = '')  # doctest: +SKIP\n",
    "\n",
    "plt.savefig(\"KNN.png\",dpi=300) \n",
    "plt.show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(val_label, model.predict(val_fea), weights=\"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "\n",
    "\n",
    "tree = XGBClassifier(random_state=1997)\n",
    "tree.fit(train_fea, train_label)\n",
    "predictions = tree.predict(val_fea) \n",
    "print(classification_report(val_label, predictions)) \n",
    "class_names = [\"normal\",\"bening\",\"malign\"]\n",
    "\n",
    "plot_confusion_matrix(tree, val_fea,val_label,display_labels=class_names,\n",
    "                                 cmap=plt.cm.Purples,values_format = '') \n",
    "\n",
    "plt.savefig(\"Tree1.png\",dpi=300) \n",
    "plt.show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(val_label, tree.predict(val_fea), weights=\"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# correlation between test harness and ideal test condition\n",
    "from numpy import mean\n",
    "from numpy import isnan\n",
    "from numpy import asarray\n",
    "from numpy import polyfit\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\"\"\"\n",
    "# create the dataset\n",
    "def get_dataset():\n",
    "\tX, y = train_fea,train_label\n",
    "\treturn X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = list()\n",
    "    #models.append(LogisticRegression())\n",
    "    models.append(RidgeClassifier())\n",
    "    models.append(SGDClassifier())\n",
    "    models.append(PassiveAggressiveClassifier())\n",
    "    models.append(KNeighborsClassifier())\n",
    "    models.append(DecisionTreeClassifier())\n",
    "    models.append(ExtraTreeClassifier())\n",
    "    models.append(LinearSVC())\n",
    "    models.append(SVC())    \n",
    "    models.append(GaussianNB())\n",
    "    models.append(RandomForestClassifier())\n",
    "    \n",
    "\n",
    "    return models\n",
    "\n",
    "# evaluate the model using a given test condition\n",
    "def evaluate_model(cv, model):\n",
    "    # get the dataset\n",
    "    X, y = get_dataset()\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # return scores\n",
    "    return mean(scores)\n",
    "\n",
    "# define test conditions\n",
    "ideal_cv = LeaveOneOut()\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# get the list of models to consider\n",
    "models = get_models()\n",
    "# collect results\n",
    "ideal_results, cv_results = list(), list()\n",
    "# evaluate each model\n",
    "for model in models:\n",
    "\t# evaluate model using each test condition\n",
    "\tcv_mean = evaluate_model(cv, model)\n",
    "\tideal_mean = evaluate_model(ideal_cv, model)\n",
    "\t# check for invalid results\n",
    "\tif isnan(cv_mean) or isnan(ideal_mean):\n",
    "\t\tcontinue\n",
    "\t# store results\n",
    "\tcv_results.append(cv_mean)\n",
    "\tideal_results.append(ideal_mean)\n",
    "\t# summarize progress\n",
    "\tprint('>%s: ideal=%.3f, cv=%.3f' % (type(model).__name__, ideal_mean, cv_mean))\n",
    "# calculate the correlation between each test condition\n",
    "corr, _ = pearsonr(cv_results, ideal_results)\n",
    "print('Correlation: %.3f' % corr)\n",
    "# scatter plot of results\n",
    "pyplot.scatter(cv_results, ideal_results)\n",
    "# plot the line of best fit\n",
    "coeff, bias = polyfit(cv_results, ideal_results, 1)\n",
    "line = coeff * asarray(cv_results) + bias\n",
    "pyplot.plot(cv_results, line, color='r')\n",
    "# label the plot\n",
    "pyplot.title('10-fold CV vs LOOCV Mean Accuracy')\n",
    "pyplot.xlabel('Mean Accuracy (10-fold CV)')\n",
    "pyplot.ylabel('Mean Accuracy (LOOCV)')\n",
    "# show the plot\n",
    "pyplot.savefig(\"sonuceff0.png\",dpi=300)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise correlated features:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# I will build a correlation matrix, which examines the \n",
    "# correlation of all features (that is, for all possible feature combinations)\n",
    "# and then visualise the correlation matrix using a heatmap\n",
    "\n",
    "# the default correlation method of pandas.corr is pearson\n",
    "# I include it anyways for the demo\n",
    "a=pd.DataFrame(train_fea)\n",
    "corrmat = a.corr(method='pearson')\n",
    "\n",
    "# we can make a heatmap with the package seaborn\n",
    "# and customise the colours of searborn's heatmap\n",
    "cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "# some more parameters for the figure\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11,11)\n",
    "\n",
    "# and now plot the correlation matrix\n",
    "sns.heatmap(corrmat, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(style='darkgrid')\n",
    "\n",
    "from sklearn.feature_selection import RFECV, VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "#from ml_metrics import quadratic_weighted_kappa\n",
    "random_state = 42\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.DataFrame(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_y_mean_dict = {}\n",
    "\n",
    "#transform the categorial columns into numeric\n",
    "for col in X_train.select_dtypes(include='object').columns:\n",
    "    ordinal_y_mean_dict[col]  = {index:i for i, index in enumerate(train_df.groupby(col)['Response'].mean().sort_values().index)}\n",
    "    print(ordinal_y_mean_dict[col])\n",
    "    X_train[col] = X_train[col].map(ordinal_y_mean_dict[col])\n",
    "    \n",
    "#check missing data\n",
    "for col in X_train:\n",
    "    if pd.isnull(X_train[col]).any():\n",
    "        print('containing NA values:', col)\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train2 = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "#check missing data\n",
    "for col in X_train2:\n",
    "    if pd.isnull(X_train2[col]).any():\n",
    "        print('containing NA values:', col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2_sup = X_train.copy() #deep copy\n",
    "\n",
    "\n",
    "X_model, X_valid, y_model, y_valid = train_test_split(X_train2_sup, y_train1, stratify=y_train1, random_state=random_state, test_size=.8)\n",
    "\n",
    "model_dict = {'LogisticRegression': LogisticRegression(penalty='l1', solver='saga', C=2, multi_class='multinomial', n_jobs=-1, random_state=random_state)\n",
    "             , 'ExtraTreesClassifier': ExtraTreesClassifier(n_estimators=200, max_depth=3, min_samples_leaf=.06, n_jobs=-1, random_state=random_state)\n",
    "              , 'RandomForestClassifier': RandomForestClassifier(n_estimators=20, max_depth=2, min_samples_leaf=.1, random_state=random_state, n_jobs=-1)\n",
    "             }\n",
    "estimator_dict = {}\n",
    "importance_fatures_sorted_all = pd.DataFrame()\n",
    "for model_name, model in model_dict.items():\n",
    "    print('='*10, model_name, '='*10)\n",
    "    model.fit(X_model, y_model)\n",
    "    print('Accuracy in training:', accuracy_score(model.predict(X_model), y_model))\n",
    "    print('Accuracy in valid:', accuracy_score(model.predict(X_valid), y_valid))\n",
    "    importance_values = np.absolute(model.coef_) if model_name == 'LogisticRegression' else model.feature_importances_\n",
    "    importance_fatures_sorted = pd.DataFrame(importance_values.reshape([-1, len(X_train2_sup.columns)]), columns=X_train2_sup.columns).mean(axis=0).sort_values(ascending=False).to_frame()\n",
    "    importance_fatures_sorted.rename(columns={0: 'feature_importance'}, inplace=True)\n",
    "    importance_fatures_sorted['ranking']= importance_fatures_sorted['feature_importance'].rank(ascending=False)\n",
    "    importance_fatures_sorted['model'] = model_name\n",
    "    print('Show top 10 important features:')\n",
    "    display(importance_fatures_sorted.drop('model', axis=1).head(10))\n",
    "    importance_fatures_sorted_all = importance_fatures_sorted_all.append(importance_fatures_sorted)\n",
    "    estimator_dict[model_name] = model\n",
    "\n",
    "\"\"\"plt.title('Feature importance ranked by number of features by model')\n",
    "sns.lineplot(data=importance_fatures_sorted_all, x='ranking', y='feature_importance', hue='model')\n",
    "plt.xlabel(\"Number of features selected\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_fatures_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'LogisticRegression'\n",
    "number_of_features = 60\n",
    "selected_features_by_model = importance_fatures_sorted_all[importance_fatures_sorted_all['model'] == selected_model].index[:number_of_features].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#it takes much more time comparing to the feature selelction by model\n",
    "rfecv = RFECV(estimator=model_dict['LogisticRegression'].set_params(max_iter=150, C=1), step=1, cv=StratifiedShuffleSplit(1, test_size=.2, random_state=random_state), scoring='accuracy', n_jobs=-1)\n",
    "rfecv.fit(X_train2_sup[selected_features_by_model], y_train1)\n",
    "plt.figure()\n",
    "plt.title('Feature importance ranked by number of features by model')\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.plot(rfecv.n_features_, rfecv.grid_scores_[rfecv.n_features_-1], marker='o', label='Optimal number of feature')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features_by_model\n",
    "selected_features_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv_df = pd.DataFrame({'col': selected_features_by_model})\n",
    "rfecv_df['rank'] = np.nan\n",
    "for index, support in enumerate(rfecv.get_support(indices=True)):\n",
    "    rfecv_df.loc[support, 'rank'] = index\n",
    "for index, rank in enumerate(rfecv.ranking_ -2):\n",
    "    if rank >= 0:\n",
    "        rfecv_df.loc[index, 'rank'] = rfecv.n_features_ + rank\n",
    "rfecv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=f;\n",
    "y=y_train1\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "chi2_selector = SelectKBest(chi2, k=100)\n",
    "X_kbest = chi2_selector.fit_transform(X, y)\n",
    "# View results\n",
    "print('Original number of features:', X.shape[1])\n",
    "print('Reduced number of features:', X_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_test(X, Y):\n",
    "    X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, \n",
    "                                                        test_size = 0.30, \n",
    "                                                        random_state = 101)\n",
    "    start = time.process_time()\n",
    "    trainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train,Y_Train)\n",
    "    print(time.process_time() - start)\n",
    "    predictionforest = trainedforest.predict(X_Test)\n",
    "    print(confusion_matrix(Y_Test,predictionforest))\n",
    "    print(classification_report(Y_Test,predictionforest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_test(f, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=100,svd_solver='full')\n",
    "X_pca = pca.fit_transform(f)\n",
    "print(pca.explained_variance_)\n",
    "\n",
    "forest_test(X_pca, y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "ica = FastICA(n_components=100)\n",
    "X_ica = ica.fit_transform(f)\n",
    "\n",
    "forest_test(X_ica, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "\n",
    "# run an LDA and use it to transform the features\n",
    "X_lda = lda.fit(f, y_train1).transform(f)\n",
    "print('Original number of features:', f.shape[1])\n",
    "print('Reduced number of features:', X_lda.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_test(X_lda, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "embedding = LocallyLinearEmbedding(n_components=100)\n",
    "X_lle = embedding.fit_transform(f)\n",
    "\n",
    "forest_test(X_lle, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "start = time.process_time()\n",
    "tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300)\n",
    "X_tsne = tsne.fit_transform(f)\n",
    "print(time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_test(X_tsne, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_selector(X, y,num_feats):\n",
    "    cor_list = []\n",
    "    feature_name = X.columns.tolist()\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    return cor_support, cor_feature\n",
    "\n",
    "cor_support, cor_feature = cor_selector(f, y_train1,100)\n",
    "print(str(len(cor_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate RFE for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "# define dataset\n",
    "\n",
    "# create pipeline\n",
    "rfe = RFE(estimator=LinearSVC(), n_features_to_select=100)\n",
    "model = DecisionTreeClassifier()\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, f, y_train1, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the algorithm wrapped by RFE\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = f,y_train1\n",
    "\treturn X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\t\n",
    "\t\n",
    "\t# cart\n",
    "\trfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=100)\n",
    "\tmodel = SVC()\n",
    "\tmodels['cart'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "\t# rf\n",
    "\trfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=100)\n",
    "\tmodel = SVC()\n",
    "\tmodels['rf'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "\t\n",
    "\treturn models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
